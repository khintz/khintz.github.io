
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://khintz.github.io/topics/dataassimilation/">
      
      
        <link rel="prev" href="../../personal/publications/">
      
      
      
      <link rel="icon" href="../../images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Data Assimilation - </title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
      
      
    
  
  
  <style>:root{--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M0%208a8%208%200%201%201%2016%200A8%208%200%200%201%200%208m8-6.5a6.5%206.5%200%201%200%200%2013%206.5%206.5%200%200%200%200-13M6.5%207.75A.75.75%200%200%201%207.25%207h1a.75.75%200%200%201%20.75.75v2.75h.25a.75.75%200%200%201%200%201.5h-2a.75.75%200%200%201%200-1.5h.25v-2h-.25a.75.75%200%200%201-.75-.75M8%206a1%201%200%201%201%200-2%201%201%200%200%201%200%202%22/%3E%3C/svg%3E');}</style>



    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#data-assimilation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="" class="md-header__button md-logo" aria-label="" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Data Assimilation
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



  

<nav class="md-nav md-nav--primary md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="" class="md-nav__button md-logo" aria-label="" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Welcome
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Personal
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Personal
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../personal/publications/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Publications
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Data Assimilation
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Data Assimilation
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-the-optimalbest-analysis-in-operational-nwp" class="md-nav__link">
    <span class="md-ellipsis">
      What is the optimal/best analysis in operational NWP?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jargon-in-da" class="md-nav__link">
    <span class="md-ellipsis">
      Jargon in DA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#observation-coveragedensity" class="md-nav__link">
    <span class="md-ellipsis">
      Observation coverage/density
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-refresher-on-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      A refresher on statistics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#framework-of-da" class="md-nav__link">
    <span class="md-ellipsis">
      Framework of DA
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reality-bites" class="md-nav__link">
    <span class="md-ellipsis">
      Reality bites
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#approximate-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Approximate solutions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#variational-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Variational Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variational Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-cost-function" class="md-nav__link">
    <span class="md-ellipsis">
      The cost function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability-distributions-for-the-multivariate-case" class="md-nav__link">
    <span class="md-ellipsis">
      Probability distributions for the multivariate case
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Probability distributions for the multivariate case">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#notation" class="md-nav__link">
    <span class="md-ellipsis">
      Notation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cost-function-multivariate-case" class="md-nav__link">
    <span class="md-ellipsis">
      Cost function multivariate case
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solving-for-the-gradient-of-the-cost-function" class="md-nav__link">
    <span class="md-ellipsis">
      Solving for the gradient of the cost function
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kalman-based-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Kalman-based Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kalman-based Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dimension" class="md-nav__link">
    <span class="md-ellipsis">
      Dimension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derivation-of-the-weight" class="md-nav__link">
    <span class="md-ellipsis">
      Derivation of the weight
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#understanding-the-kalman-gain" class="md-nav__link">
    <span class="md-ellipsis">
      Understanding the Kalman gain
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-kalman-filter-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      The Kalman Filter Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimal-interpolation-oi-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Optimal Interpolation (OI) equations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#characteristic-overview-of-da-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Characteristic overview of DA methods
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="data-assimilation">Data Assimilation<a class="headerlink" href="#data-assimilation" title="Permanent link">&para;</a></h1>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Data Assimilation (DA) is the art of combining information from different sources in an optimal way. In climate and NWP, these sources are a first guess and observations with the aim of improving an estimate of the state of a system.
DA has two main objectives.</p>
<p>1) Produce an initial state from which we can start our forecast.
2) Quantifying the uncertainty of the initial state.</p>
<p>From the initial state of the atmosphere the model integrates a set of prognostic equations forward in time. This is prediction.</p>
<h3 id="what-is-the-optimalbest-analysis-in-operational-nwp">What is the optimal/best analysis in operational NWP?<a class="headerlink" href="#what-is-the-optimalbest-analysis-in-operational-nwp" title="Permanent link">&para;</a></h3>
<ul>
<li>The best analysis is the one that leads to the best forecast, not necessarily the analysis closest to the true state.</li>
<li>Computational efficient: Depending on operational setup you should make your analysis in 30-60 minutes. Even your smartphones can compute a reasonable weather forecast faster than the weather evolves.</li>
<li>Minimises error/maximises probability.</li>
</ul>
<h3 id="jargon-in-da">Jargon in DA<a class="headerlink" href="#jargon-in-da" title="Permanent link">&para;</a></h3>
<p>Our prior information is a First Guess (<span class="arithmatex">\(\mathbf{x_g}\)</span>) and some Observations (<span class="arithmatex">\(\mathbf{y}\)</span>). <span class="arithmatex">\(\mathbf{x_g}\)</span> is the prior information and can for example be:</p>
<ul>
<li>A random guess (bad choice!)</li>
<li>A climatological field (better, but room for improvement)</li>
<li>The analysis from a previous cycle
∗ A short term forecast from the previous cycle <span class="arithmatex">\(\mathbf{x_b}\)</span></li>
<li>Most often <span class="arithmatex">\(\mathbf{x_g}=\mathbf{x_b}\)</span> in operational setups.</li>
</ul>
<p>We shall refer to a short term forecast as the background field <span class="arithmatex">\(\mathbf{x_b}\)</span>, and denote our analysis as <span class="arithmatex">\(\mathbf{x_a}\)</span></p>
<h3 id="observation-coveragedensity">Observation coverage/density<a class="headerlink" href="#observation-coveragedensity" title="Permanent link">&para;</a></h3>
<p>The DMI Harmonie model, "NEA", has about <span class="arithmatex">\(10^9\)</span> prognostic variables that we need to assign an initial value to produce a forecast.
We have far less observations (<span class="arithmatex">\(\approx 10^4\)</span> to <span class="arithmatex">\(10^6\)</span>) giving rise to an insufficient data coverage. Furthermore, observations are:</p>
<ul>
<li>Not evenly distributed in space and time</li>
<li>Not taken exactly at the grid points.</li>
<li>Not perfect - they contain errors. Quality control is needed.</li>
</ul>
<p>An observation operator is needed to interpolate and convert to state variables if needed.</p>
<h3 id="a-refresher-on-statistics">A refresher on statistics<a class="headerlink" href="#a-refresher-on-statistics" title="Permanent link">&para;</a></h3>
<p>Suppose we have the noon-day pressure <span class="arithmatex">\(p_i\)</span> and temperature <span class="arithmatex">\(T_i\)</span> at Copenhagen, every day for a year. Let <span class="arithmatex">\(n=365\)</span>.
The mean pressure, <span class="arithmatex">\(\overline{p}\)</span>, is defined to be</p>
<div class="arithmatex">\[
\overline{p}=\mathbb{E}(p)=\frac{1}{n}\sum_{i=1}^{n}p_i
\]</div>
<p>and similarly for the mean temperature <span class="arithmatex">\(\overline{T}\)</span>.</p>
<p>The variance of pressure, <span class="arithmatex">\(\sigma_p^2\)</span>, is defined as</p>
<div class="arithmatex">\[
\sigma_p^2=\mathbb{E}((p-\overline{p})^2)=\frac{1}{n}\sum_{i=1}^{n}(p_i-\overline{p})^2
\]</div>
<p>and similarly for variance of the temperature <span class="arithmatex">\(\sigma_T^2\)</span>.</p>
<p>The standard deviations, <span class="arithmatex">\(\sigma_p\)</span> and <span class="arithmatex">\(\sigma_T\)</span> are the square roots of the variances. They measure the root mean square deviation from the mean.</p>
<h3 id="framework-of-da">Framework of DA<a class="headerlink" href="#framework-of-da" title="Permanent link">&para;</a></h3>
<p>As has been described we have information both from a short term forecast, <span class="arithmatex">\(\mathbf{x_b}\)</span>, and some observations <span class="arithmatex">\(\mathbf{y}\)</span>. We need to develop a framework for combining the two sources of information.</p>
<p>Luckily this framework already exist! → <mark class="critic">Bayes Theorem</mark></p>
<div class="arithmatex">\[
\text{pdf}(\mathbf{x}|\mathbf{y})=\frac{\text{pdf}(\mathbf{y}|\mathbf{x})\text{pdf}(\mathbf{x})}{\text{pdf}(\mathbf{y})}
\]</div>
<ul>
<li><span class="arithmatex">\(\text{pdf}(\mathbf{x}|\mathbf{y})\)</span> is the posterior probability density function (pdf) of <span class="arithmatex">\(\mathbf{x}\)</span> given <span class="arithmatex">\(\mathbf{y}\)</span></li>
<li><span class="arithmatex">\(\text{pdf}(\mathbf{y}|\mathbf{x})\)</span> is the likelihood function, the <span class="arithmatex">\(\text{pdf}\)</span> of the observations given the state variables.</li>
<li><span class="arithmatex">\(\text{pdf}(\mathbf{x})\)</span> is the prior <span class="arithmatex">\(\text{pdf}\)</span> of the state variables coming from the background field (model)</li>
<li><span class="arithmatex">\(\text{pdf}(\mathbf{y})\)</span> is the evidence. It is used as a normalisation constant and often not computed explicitly so we will ignore this for now.</li>
</ul>
<p>Combining information in the form of the prior and the likehood gives us a more narrow posterior <span class="arithmatex">\(\text{pdf}\)</span>. Note that as long one knows the associated error of either the model or observations the posterior will always gain information.</p>
<figure>
<p><img alt="Image title" src="/images/prob_distribution.png" width="600" />
  </p>
<figcaption>Probability Density Functions</figcaption>
</figure>
<h3 id="reality-bites">Reality bites<a class="headerlink" href="#reality-bites" title="Permanent link">&para;</a></h3>
<p>Making no approximations - considering the full non-linear DA problem - we have to find the joint <span class="arithmatex">\(\text{pdf}\)</span> of all variables, that is the probabilities for all possible combinations of all of our variables.</p>
<p>Assume we only need 10 bins for each variable to generate a joint <span class="arithmatex">\(\text{pdf}\)</span> and assume we have a small model of only <span class="arithmatex">\(10^6\)</span> variables. Then we need to store <span class="arithmatex">\(10^{1000000}\)</span> numbers.</p>
<p>There are approximately <span class="arithmatex">\(10^{80}\)</span> atoms in the universe<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.
So data assimilation is much larger than the universe!
We need <span class="arithmatex">\(10^{52}\)</span> solar system sized hard drives to store just a googol (<span class="arithmatex">\(10^{100}\)</span>) bytes, but we only have about <span class="arithmatex">\(10^{24}\)</span> stars<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. Approximations and optimisations are indeed needed.</p>
<p>The key here is that we can’t determine the pdf’s in large dimensional systems.</p>
<h3 id="approximate-solutions">Approximate solutions<a class="headerlink" href="#approximate-solutions" title="Permanent link">&para;</a></h3>
<p>Estimating the <span class="arithmatex">\(\text{pdf}\)</span>’s in large dimensional systems, such as in NWP, is practically impossible!</p>
<p>Approximate solutions of Bayes theorem leads to data assimilation methods.</p>
<ul>
<li><mark class="critic">Variational methods</mark>: Solves for the mode of the posterior pdf.</li>
<li><mark class="critic">Kalman-based methods</mark>: Solves for the mean and covariance of posterior pdf.</li>
<li><mark class="critic">Particle Filters</mark>: Finds a sample representaion of the posterior pdf.</li>
</ul>
<p><mark class="critic">Variational methods</mark> and <mark class="critic">Kalman-based methods</mark> both assume errors to be Gaussian. <mark class="critic">Particle Filters</mark>, however have no such assumptions.</p>
<h2 id="variational-methods">Variational Methods<a class="headerlink" href="#variational-methods" title="Permanent link">&para;</a></h2>
<p>The variational methods assume errors to be Gaussian. This is convenient as the pdf is completely determined by the mean and covariance. But it is also a strong constraint. Do the errors, in fact, follow a Gaussian?</p>
<p>In geophysics, we often deal with only the positive real axis, where we then often find a tail on the error distribution. For example precipitation, wind, salinity etc.</p>
<p>Let us see how the variational approach works with a scalar example of temperature observations.</p>
<p>We will feed our own ”toy data assimilation system” the observations, to obtain the most likely temperature given your observations.</p>
<h3 id="the-cost-function">The cost function<a class="headerlink" href="#the-cost-function" title="Permanent link">&para;</a></h3>
<p>We assume that your observations has been drawn from a Gaussian distribution.</p>
<div class="arithmatex">\[
p(T_1|T)=\frac{1}{\sqrt{2\pi\sigma_1^2}}\exp\left(-\frac{(T_1-T)^2}{2\sigma_1^2}\right)
\]</div>
<p>and likewise for <span class="arithmatex">\(T_2\)</span>. We can express the joint probability by multiplying the two probabilities together.</p>
<div class="arithmatex">\[
p(T_1,T_2|T)=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\frac{(T_1-T)^2}{2\sigma_1^2}-\frac{(T_2-T)^2}{2\sigma_2^2}\right)
\]</div>
<p>This is the same as the likelihood for <span class="arithmatex">\(T\)</span> given <span class="arithmatex">\(T_1\)</span> and <span class="arithmatex">\(T_2\)</span>, <span class="arithmatex">\(L(T|T_1,T_2)\)</span>. To find the most likely <span class="arithmatex">\(T\)</span>, which will be our analysis temperature <span class="arithmatex">\(T_a\)</span>, we want to maximise the likelihood given <span class="arithmatex">\(T_1\)</span> and <span class="arithmatex">\(T_2\)</span>.</p>
<div class="arithmatex">\[
\text{max}[L(T|T_1,T_2)]=\text{max}\left[\frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\frac{(T_1-T)^2}{2\sigma_1^2}-\frac{(T_2-T)^2}{2\sigma_2^2}\right)\right]
\]</div>
<p>To make things easier, we take the logarithmic of this expression. Note that the logarithmic is a monotonic function, so maximising the logarithmic of the function is equivalent to maximising the function itself.</p>
<div class="arithmatex">\[
\text{max}[\ln L(T|T_1,T_2)]=\text{max}\left[\text{const}-\frac{1}{2}\left(\frac{(T-T_1)^2}{\sigma_1^2}+\frac{(T-T_2)^2}{\sigma_2^2}\right)\right]
\]</div>
<p>Note that maximising the function is equivalent to minimising the last term on the right-hand-side. For our ”two-temperature-problem” this is defined as our cost function.</p>
<div class="arithmatex">\[
J=\frac{1}{2}\left(\frac{(T-T_1)^2}{\sigma_1^2}+\frac{(T-T_2)^2}{\sigma_2^2}\right)
\]</div>
<p>Minimising <span class="arithmatex">\(J\)</span> corresponds to maximising the likelihood of <span class="arithmatex">\(T\)</span> given <span class="arithmatex">\(T_1\)</span> and <span class="arithmatex">\(T_2\)</span>.</p>
<p>To minimise <span class="arithmatex">\(J\)</span> to find our analysis temperature using the variational approach, we will start by guessing some value of <span class="arithmatex">\(T\)</span> and explore space. Different algorithms exist for this such as "steepest-descent".</p>
<details class="example">
<summary>Example</summary>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&#39;Toy-Tool&#39; to play with a very simple scalar case of the cost function</span>
<span class="sd">in variational data assimilation.</span>

<span class="sd">Given two guesses of temperature, we try to find the most likely</span>
<span class="sd">true temperature by minimizing a cost function.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Define variables</span>
<span class="n">T1</span> <span class="o">=</span> <span class="mf">21.6</span>  <span class="c1"># First guess of temperature</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="mf">1.8</span>  <span class="c1"># Standard deviation for T1</span>

<span class="n">T2</span> <span class="o">=</span> <span class="mf">23.4</span>  <span class="c1"># Second guess of temperature</span>
<span class="n">sigma2</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># Standard deviation for T2</span>

<span class="c1"># Initial guess (mean of T1 and T2)</span>
<span class="n">T0</span> <span class="o">=</span> <span class="p">(</span><span class="n">T1</span> <span class="o">+</span> <span class="n">T2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="c1"># Initialize variables</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">T0</span>  <span class="c1"># Current temperature guess</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Epsilon for perturbation</span>
<span class="n">J0</span> <span class="o">=</span> <span class="mf">2000.</span>  <span class="c1"># Initial cost value</span>

<span class="c1"># Lists to store iteration data</span>
<span class="n">iteration</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Ta</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">costF</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Perform 100 iterations</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">direction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="n">size_direction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">direction</span>

    <span class="n">Tg</span> <span class="o">=</span> <span class="n">T</span> <span class="o">+</span> <span class="n">size_direction</span>  <span class="c1"># New temperature guess</span>
    <span class="n">J</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="p">((</span><span class="n">Tg</span> <span class="o">-</span> <span class="n">T1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">Tg</span> <span class="o">-</span> <span class="n">T2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">sigma2</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Cost function</span>

    <span class="c1"># Update if new cost is lower</span>
    <span class="k">if</span> <span class="n">J</span> <span class="o">&lt;</span> <span class="n">J0</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">J0</span> <span class="o">=</span> <span class="n">J</span>
        <span class="n">T</span> <span class="o">=</span> <span class="n">Tg</span>
        <span class="n">iteration</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="n">Ta</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
        <span class="n">costF</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">J</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ta</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">invert_yaxis</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;T [C]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
</details>
<h3 id="probability-distributions-for-the-multivariate-case">Probability distributions for the multivariate case<a class="headerlink" href="#probability-distributions-for-the-multivariate-case" title="Permanent link">&para;</a></h3>
<p>The principles we have just seen, are the same in the multivariate case.</p>
<p>The <mark class="critic">prior</mark></p>
<div class="arithmatex">\[
p(\mathbf{x})\propto\exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{x_b})^T\mathbf{P}^{-1}(\mathbf{x}-\mathbf{x_b})\right)
\]</div>
<p>The <mark class="critic">likelihood</mark></p>
<div class="arithmatex">\[
p(\mathbf{y}|\mathbf{x})\propto\exp\left(-\frac{1}{2}(\mathbf{y}-H(\mathbf{x}))^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}))\right)
\]</div>
<p>The <mark class="critic">posterior</mark></p>
<div class="arithmatex">\[
p(\mathbf{x}|\mathbf{y})\propto\exp\left(-\frac{1}{2}[(\mathbf{x}-\mathbf{x_b})^T\mathbf{P}_b^{-1}(\mathbf{x}-\mathbf{x_b})+(\mathbf{y}-H(\mathbf{x}))^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}))]\right)
\]</div>
<h4 id="notation">Notation<a class="headerlink" href="#notation" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex">\(\mathbf{x}\)</span>: State vector of size <span class="arithmatex">\(n\)</span></p>
<p><span class="arithmatex">\(\mathbf{x_b}\)</span>: Background state vector of size <span class="arithmatex">\(n\)</span></p>
<p><span class="arithmatex">\(\mathbf{P}_b\)</span> or <span class="arithmatex">\(\mathbf{B}\)</span>: Background error covariance matrix of size <span class="arithmatex">\(n\times n\)</span></p>
<p><span class="arithmatex">\(\mathbf{y}\)</span>: Observation vector of size <span class="arithmatex">\(p\)</span></p>
<p><span class="arithmatex">\(\mathbf{R}\)</span>: Observation error covariance matrix of size <span class="arithmatex">\(p\times p\)</span></p>
<p><span class="arithmatex">\(n\)</span>: Total number of grid points <span class="arithmatex">\(\times\)</span> number of model variables (<span class="arithmatex">\(\approx 10^7\)</span>)</p>
<p><span class="arithmatex">\(p\)</span>: Total number of observations (<span class="arithmatex">\(\approx 10^4\)</span>)</p>
<p><span class="arithmatex">\(H(\mathbf{x})\)</span>: Observation operator that maps model space to observation space. <span class="arithmatex">\(H(x_i)\)</span> is the models estimate on <span class="arithmatex">\(y_i\)</span>. <span class="arithmatex">\(H\)</span> can be non-linear (eg. radiance measurements) or linear (eq. synop temperature measurements).</p>
<h3 id="cost-function-multivariate-case">Cost function multivariate case<a class="headerlink" href="#cost-function-multivariate-case" title="Permanent link">&para;</a></h3>
<p>Taking the term in the brackets of the <mark class="critic">posterior</mark> gives us the cost function that we want to minimise to maximise the posterior.</p>
<div class="arithmatex">\[
J(\mathbf{x})=\frac{1}{2}[(\mathbf{x}-\mathbf{x_b})^T\mathbf{P}_b^{-1}(\mathbf{x}-\mathbf{x_b})+(\mathbf{y}-H(\mathbf{x}))^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}))]
\]</div>
<p>where <span class="arithmatex">\(\mathbf{x}\)</span> is the state vector with all variables. The <span class="arithmatex">\(\mathbf{x}\)</span> that minimises the cost function, <span class="arithmatex">\(J\)</span> is our analysis state vector <span class="arithmatex">\(\mathbf{x}_a\)</span>. <span class="arithmatex">\(\mathbf{P}_b\)</span> is the background error covariance matrix. <span class="arithmatex">\(\mathbf{P}_b\)</span> is sometimes also denoted <span class="arithmatex">\(\mathbf{B}\)</span> in the literature. Here I use <span class="arithmatex">\(\mathbf{P}_b\)</span> for consistency with the Kalman Filter algorithm (to be introduced)</p>
<p>The cost function cannot be computed directly. Different assumptions leads to either <mark class="critic">3DVar</mark> or <mark class="critic">4DVar</mark>.</p>
<p><span class="arithmatex">\(\mathbf{P}_b\)</span> is a huge matrix! (<span class="arithmatex">\(\approx 10^7\times10^7\)</span>) - We can’t store this on a computer, so we are forced to simplify it. Furthermore, it is not given that we have all the information needed to determine all of its elements.</p>
<p>We dont know <span class="arithmatex">\(\mathbf{P}_b\)</span>. In general <span class="arithmatex">\(\mathbf{P}_b = \text{cov}[\mathbf{x}_t − \mathbf{x}_b]\)</span>, but we have no way to know <span class="arithmatex">\(\mathbf{x}_t\)</span>, therefore a proxy is needed for <span class="arithmatex">\(\mathbf{x}_t\)</span>.</p>
<p>As a proxy for <span class="arithmatex">\(\mathbf{x}_t\)</span> one can use ”observation-minus-background” statistics, by running the model for a long period and see how the error looks in average.</p>
<p><span class="arithmatex">\(\mathbf{P}_b\)</span> is essential. Its role is to spread out information from observations. How should a pressure observation in Copenhagen affect variables in Oslo? Also, it ensures dynamically consistent increments in other model variables (How should a temperature increase affect the wind?)</p>
<details class="tip">
<summary>Tip</summary>
<p>Good To Know: <span class="arithmatex">\(\mathbf{P}_b\)</span> is often referred to as ”structure functions” in the literature.</p>
</details>
<figure>
<p><img alt="Image title" src="/images/single_obs.png" width="400" />
  </p>
<figcaption>Increment of assimilating a single pressure observation</figcaption>
</figure>
<p>In the figure the difference (increment) between to analysis states is shown. A single additional pressure observation is assimilated in the second analysis. The increment is largest close to the observation and decreases with distance. The spread of the increment is determined by <span class="arithmatex">\(\mathbf{P}_b\)</span>.</p>
<h3 id="solving-for-the-gradient-of-the-cost-function">Solving for the gradient of the cost function<a class="headerlink" href="#solving-for-the-gradient-of-the-cost-function" title="Permanent link">&para;</a></h3>
<p>The minimum of the cost function is obtained for <span class="arithmatex">\(\mathbf{x}=\mathbf{x}_a\)</span>, e.g. the solution of</p>
<div class="arithmatex">\[
\nabla J(\mathbf{x}_a)=0
\]</div>
<p>So we wish to solve for the gradient of the cost function. To simplify the problem we linearise the observation operator <span class="arithmatex">\(H\)</span> as</p>
<div class="arithmatex">\[
H(\mathbf{x})\approx \nabla H(\mathbf{x}_b)\cdot\delta \mathbf{x}+H(\mathbf{x}_b)
\]</div>
<p>and assume that the analysis is close to the truth so that we can write</p>
<div class="arithmatex">\[
x=\mathbf{x}_b+(\mathbf{x}-\mathbf{x}_b)
\]</div>
<p>assuming <span class="arithmatex">\(\mathbf{x}-\mathbf{x}_b\)</span> is small, <span class="arithmatex">\(\mathbf{y}-H(\mathbf{x})\)</span> can be written as</p>
<div class="arithmatex">\[
\begin{align*}
[\mathbf{y}-H(\mathbf{x})]=\mathbf{y}-H(\mathbf{x}_b+(\mathbf{x}-\mathbf{x}_b))\\
=\mathbf{y}-H(\mathbf{x})-\mathbf{H}(\mathbf{x}-\mathbf{x}_b)
\end{align*}
\]</div>
<p>This is an advantage as <span class="arithmatex">\(H(\mathbf{x}_b)\)</span> and <span class="arithmatex">\(\mathbf{x}\)</span> is known a priori. The cost function can now be written as</p>
<div class="arithmatex">\[
\begin{align*}
J(\mathbf{x})=&amp;\frac{1}{2}[(\mathbf{x}-\mathbf{x}_b)^T\mathbf{P}_b^{-1}(\mathbf{x}-\mathbf{x}_b)+[(\mathbf{y}-H(\mathbf{x}_b)-\mathbf{H}(\mathbf{x}-\mathbf{x}_b)]^T\mathbf{R}^{-1} \\
&amp;[(\mathbf{y}-H(\mathbf{x}_b)-\mathbf{H}(\mathbf{x}-\mathbf{x}_b)])]]
\end{align*}
\]</div>
<p>If we then assume that <span class="arithmatex">\(\mathbf{R}\)</span> is symmetric so that <span class="arithmatex">\(\mathbf{HR}^{-1}=\mathbf{R}^{-1}\mathbf{H}\)</span> and expanding the brackets we get</p>
<div class="arithmatex">\[
\begin{align*}
J(\mathbf{x})=&amp;\frac{1}{2}[(\mathbf{x}-\mathbf{x}_b)^T\mathbf{P}_b^{-1}(\mathbf{x}-\mathbf{x}_b)+(\mathbf{x}-\mathbf{x}_b)^T\mathbf{H}^T\mathbf{R}^{-1}\mathbf{H}(\mathbf{x}-\mathbf{x}_b) \\
&amp;-(\mathbf{y}-H(\mathbf{x}_b))^T\mathbf{R}^{-1}\mathbf{H}(\mathbf{x}-\mathbf{x}_b)-(\mathbf{x}-\mathbf{x}_b)^T\mathbf{H}^T\mathbf{R}^{-1} \\
&amp;(\mathbf{y}-H(\mathbf{x}_b))+(\mathbf{y}-H(\mathbf{x}_b))^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}_b))]
\end{align*}
\]</div>
<p>If we combine the first two terms we get</p>
<div class="arithmatex">\[
\begin{align*}
2J(\mathbf{x})=&amp;\ (\mathbf{x}-\mathbf{x}_b)^T[\mathbf{P}_b^{-1}+\mathbf{H}^T\mathbf{R}^{-1}\mathbf{H}](\mathbf{x}-\mathbf{x}_b) \\
&amp;-(\mathbf{y}-H(\mathbf{x}_b))^T\mathbf{R}^{-1}\mathbf{H}(\mathbf{x}-\mathbf{x}_b) \\
&amp;-(\mathbf{x}-\mathbf{x}_b)^T\mathbf{H}^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}_b)) \\
&amp;+\text(Term\ independent\ on\ \mathbf{x}) \\
=&amp;\ (\mathbf{x}-\mathbf{x}_b)^T[\mathbf{P}_b^{-1}+\mathbf{H}^T\mathbf{R}^{-1}\mathbf{H}](\mathbf{x}-\mathbf{x}_b) \\
&amp;-2(\mathbf{y}-H(\mathbf{x}_b))^T\mathbf{R}^{-1}\mathbf{H}(\mathbf{x}-\mathbf{x}_b) \\
&amp;+\text(Term\ independent\ on\ \mathbf{x})
\end{align*}
\]</div>
<p>Given a quadratic function <span class="arithmatex">\(F(\mathbf{x})=\frac{1}{2}\mathbf{x}^T\mathbf{Ax}+\mathbf{d}^T\mathbf{x}+c\)</span> the gradient is given by <span class="arithmatex">\(\nabla F(\mathbf{x})=\mathbf{Ax}+\mathbf{d}\)</span>. Using this and setting <span class="arithmatex">\(\nabla J(\mathbf{x})=0\)</span> to ensure <span class="arithmatex">\(J\)</span> is a minimum (though it could be a maximum or a saddle point) we obtain an analytical expression for the analysis state vector <span class="arithmatex">\(\mathbf{x}_a\)</span>.</p>
<div class="arithmatex">\[
\begin{align*}
\mathbf{x}_a = \mathbf{x}_b+(\mathbf{P}_b^{-1}+\mathbf{H}^T\mathbf{R}^{-1}\mathbf{H})^{-1}\mathbf{H}^T\mathbf{R}^{-1}(\mathbf{y}-H(\mathbf{x}_b))
\end{align*}
\]</div>
<p>The is the analytical solution to <mark class="critic">3DVar</mark>. In practice it is solved by an iterative method such as the steepest descent method, as we saw in the scalar example. Unfortunately for us, it is impossible to invert such huge matrices as <span class="arithmatex">\(\mathbf{P}_b\)</span> so we need to find approximations. Also <mark class="critic">3DVar</mark> assumes all observations to be taken at the time of the analysis. This is not the case in reality. <span class="arithmatex">\(\mathbf{P}_b\)</span> is also assumed to be constant in time, which is not the case in reality (not allowed to evolve dynamically).</p>
<figure>
<p><img alt="Image title" src="/images/3dvar.png" width="400" />
  </p>
<figcaption>Schematic figure of the 3DVar algorithm</figcaption>
</figure>
<h2 id="kalman-based-methods">Kalman-based Methods<a class="headerlink" href="#kalman-based-methods" title="Permanent link">&para;</a></h2>
<p>Optimal Interpolation, Kalman Filter and Kalman Ensemble filters are methods that are widely used in operational centers and they are all based on the Kalman equations, which we shall derive to broaden our understanding of the methods.</p>
<p>An analysis is found by using a least square approach in the sense that we find an ’optimal’ analysis by minimising the errors.</p>
<p>We write our analysis <span class="arithmatex">\(\mathbf{x}_a\)</span> as a linear combination of the background, <span class="arithmatex">\(\mathbf{x}_b\)</span> and some observations <span class="arithmatex">\(\mathbf{y}\)</span> as</p>
<div class="arithmatex">\[
\mathbf{x}_a=\mathbf{Lx}_b+\mathbf{Wy}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{L}\)</span> and <span class="arithmatex">\(\mathbf{W}\)</span> are weights that we need to find.</p>
<p>One can derive the least square solution from this equation and at the same time get rid of one of the weights. This is a statistical approach trying to minimise the errors of the analysis.</p>
<p>The errors of <span class="arithmatex">\(\mathbf{x}_a\)</span> and <span class="arithmatex">\(\mathbf{x}_b\)</span> can be written as</p>
<div class="arithmatex">\[
\begin{align*}
\mathbf{e}_a=&amp;\ \mathbf{x}_a-\mathbf{x}_t \\
\mathbf{e}_b=&amp;\ \mathbf{x}_b-\mathbf{x}_t
\end{align*}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{x}_t\)</span> is the true (unknown) state. A linear observation process can be defined as</p>
<div class="arithmatex">\[
\mathbf{y}=\mathbf{Hx}_t+\mathbf{b}_0
\]</div>
<p>where <span class="arithmatex">\(\mathbf{H}\)</span> is a matrix representing a linear transformation between the true variables into the observed ones (also called a forward operator) and <span class="arithmatex">\(\mathbf{b}_0\)</span> is the observational error.</p>
<p>Assume that the observation error have zero mean and covariance <span class="arithmatex">\(\mathbf{R}\)</span>,</p>
<div class="arithmatex">\[
\begin{align*}
\mathbb{E}(\mathbf{b}_0) =&amp; 0 \\
\mathbb{E}(\mathbf{b}_0\mathbf{b}_0^T) =&amp; \mathbf{R}_k\delta_{kk'}
\end{align*}
\]</div>
<p>where <span class="arithmatex">\(\delta_{kk'}\)</span> is the dirac-delta function.</p>
<p>Also assume that the observations errors and model errors are uncorrelated</p>
<div class="arithmatex">\[
\mathbb{E}(\mathbf{b}_t\mathbf{b}_0^T)=0
\]</div>
<p>Substitung the errors of the analysis and background into our analysis equation and subtracting <span class="arithmatex">\(\mathbf{x}_t\)</span> we get</p>
<div class="arithmatex">\[
\mathbf{e}_a=\underbrace{\mathbf{Le}_b}_{\text{Background Error}}+\underbrace{\mathbf{Wb}_0}_{\text{Observational Error}}+\underbrace{(\mathbf{L}+\mathbf{WH}-\mathbf{I})\mathbf{x}_t}_{\text{Bias}}
\]</div>
<p>Assuming that the forecast error is unbiased (<span class="arithmatex">\(\mathbb{E}(\mathbf{e}_b)=\mathbb{E}(\mathbf{x}_b-\mathbf{x}_t)=0\)</span>), the condition <span class="arithmatex">\((\mathbf{L}+\mathbf{WH}-\mathbf{I})\mathbb{E}(\mathbf{x}_t)=0\)</span> must be met. In general <span class="arithmatex">\(\mathbb{E}(\mathbf{x}_t)\neq 0\)</span> so to obtain an unbiased analysis we can write the first weight in terms of the second as</p>
<div class="arithmatex">\[
\mathbf{L}=\mathbf{I}-\mathbf{WH}
\]</div>
<p>Substituting this into the analysis equation we get the Kalman analysis equation</p>
<div class="arithmatex">\[
\begin{align*}
\mathbf{x}_a=&amp;\mathbf{x}_b+\mathbf{W}\underbrace{(\mathbf{y}-\mathbf{Hx}_b)}_{\text{Innovation}} \\
=&amp;\mathbf{x}_b+\mathbf{Wd}
\end{align*}
\]</div>
<h3 id="dimension">Dimension<a class="headerlink" href="#dimension" title="Permanent link">&para;</a></h3>
<p><span class="arithmatex">\(n=\text{Total number of grid points}\times\text{number of model variables}\)</span></p>
<p><span class="arithmatex">\(\mathbf{x}\)</span>: State vector of size <span class="arithmatex">\(n\)</span></p>
<p><span class="arithmatex">\(\mathbf{W}\)</span>: Weight matrix of size <span class="arithmatex">\(p\times n\)</span> where <span class="arithmatex">\(p\)</span> is the number of observations</p>
<p><span class="arithmatex">\(\mathbf{y}\)</span>: Observation vector of size <span class="arithmatex">\(p\)</span></p>
<p><span class="arithmatex">\(\mathbf{H}\)</span>: Matrix of size <span class="arithmatex">\(n\times p\)</span></p>
<h3 id="derivation-of-the-weight">Derivation of the weight<a class="headerlink" href="#derivation-of-the-weight" title="Permanent link">&para;</a></h3>
<p>At this point <span class="arithmatex">\(\mathbf{W}\)</span> is still unknown to us. <span class="arithmatex">\(\mathbf{W}\)</span> chosen such that the variances are minimised. Consider the error covariance of the analysis,</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{P}_a=\text{cov}[\mathbf{x}_t-\mathbf{x}_a]=
    \left[ {\begin{array}{cccc}
    \sigma_{1,1}^2 &amp; \sigma_{1,2}^2 &amp; \dots &amp; \sigma_{1,n}^2 \\
    \sigma_{2,1}^2 &amp; \sigma_{2,2}^2 &amp; \dots &amp; \sigma_{2,n}^2 \\
    \vdots         &amp; \vdots         &amp; \ddots &amp; \vdots \\
    \sigma_{m,1}^2 &amp; \sigma_{m,2}^2 &amp; \dots &amp; \sigma_{m,n}^2
    \end{array} } \right]
\end{align*}
\]</div>
<p>Note that the variances are the trace of the error covariance matrix. This can be expanded by using the Kalman analysis equation and <span class="arithmatex">\(\mathbf{y}=\mathbf{Hx}_t+\mathbf{b}_0\)</span> to get</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{P}_a = \text{cov}[(\mathbf{I}-\mathbf{WH})(\mathbf{x}_t-\mathbf{x}_b)-\mathbf{Wb}_0].
\end{align*}
\]</div>
<p>This can be simplified by using the covariance matrix identity, <span class="arithmatex">\(\text{cov}(\mathbf{AB})=\mathbf{A}\text{cov}(\mathbf{B})\mathbf{A}^T\)</span>:</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{P}_a = (\mathbf{I}-\mathbf{WH})\mathbf{P}_b(\mathbf{I}-\mathbf{WH})^T+\mathbf{WRW}^T,
\end{align*}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{P}_b=\text{cov}(\mathbf{x}_t-\mathbf{x}_b)\)</span> and <span class="arithmatex">\(\mathbf{R}=\text{cov}(\mathbf{b}_0)\)</span>.</p>
<p>We expand by using that <span class="arithmatex">\(\mathbf{I}=\mathbf{I}^T\)</span> and defining <span class="arithmatex">\(\mathbf{S}=\mathbf{HP}_b\mathbf{H}^T+\mathbf{R}\)</span> to get</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{P}_a = \mathbf{P}_b - \mathbf{W}^T\mathbf{H}^T\mathbf{P}_b-\mathbf{WHP}_b+\mathbf{WSW}^T.
\end{align*}
\]</div>
<p>Recall that we want to minimise the trace of the error covariance matrix (to minimise the variances). We take the derivative of the trace of <span class="arithmatex">\(\mathbf{P}_a\)</span> with respect to <span class="arithmatex">\(\mathbf{W}\)</span> and setting it equal to 0 to find the minimum (hint: Use the matrix identity <span class="arithmatex">\(\nabla_A\text{Tr}(\mathbf{AB})=\mathbf{B}^T\)</span>).</p>
<div class="arithmatex">\[
\begin{align*}
    \frac{\partial\text{Tr}(\mathbf{P}_a)}{\partial\mathbf{W}} = -2(\mathbf{HP}_b)^T + 2\mathbf{WS} \equiv 0.
\end{align*}
\]</div>
<p>Using that <span class="arithmatex">\(\mathbf{P}_b\)</span> is symetric (<span class="arithmatex">\(\mathbf{P}_b=\mathbf{P}_b^T\)</span>) and solving for <span class="arithmatex">\(\mathbf{W}\)</span> yields the optimal weight</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{W}=\mathbf{H}^T\mathbf{P}_b\mathbf{S}^{-1} = \frac{\mathbf{H}^T\mathbf{P}_b}{\mathbf{HP}_b\mathbf{H}^T+\mathbf{R}}
\end{align*}
\]</div>
<p>This is called the Kalman gain and is the optimal weight that minimises the variances of the analysis.</p>
<h3 id="understanding-the-kalman-gain">Understanding the Kalman gain<a class="headerlink" href="#understanding-the-kalman-gain" title="Permanent link">&para;</a></h3>
<p>It is important to get an intuitive understanding of the Kalman gain. The Kalman gain is a measure of how much we trust the observations. If the observation error is large, the Kalman gain will be small and vice versa. If the background error is large, the Kalman gain will be small and vice versa.</p>
<p>Recall that <span class="arithmatex">\(\mathbf{R}\)</span> is the observational error covariance matrix and <span class="arithmatex">\(\mathbf{P}_b\)</span> is the background error covariance matrix.</p>
<p>If the <mark class="critic">observational error</mark> is much larger than the <mark class="critic">model error</mark> the Kalman gain will go towards 0, making the innovation term in equation small, such that observations are given a low weight.</p>
<p>On the other hand, if the <mark class="critic">model error</mark> is much larger than the <mark class="critic">observational error</mark> the weight will go towards 1, given the innovation term a high weight in the analysis equation.</p>
<p>One advantage of the Kalman system is that we get the error of the analysis directly by computing <span class="arithmatex">\(\mathbf{P}_a\)</span>. This is not the case for the variational methods. However, we can now simplify the equation for <span class="arithmatex">\(\mathbf{P}_a\)</span> by multiplying the Kalman gain with <span class="arithmatex">\(\mathbf{W}^T\mathbf{S}\)</span> to get</p>
<div class="arithmatex">\[
\mathbf{W}^T\mathbf{SW} = \mathbf{P}_b\mathbf{H}^T\mathbf{W}^T
\]</div>
<p>Substituting this into the equation for <span class="arithmatex">\(\mathbf{P}_a\)</span> we get</p>
<div class="arithmatex">\[
\mathbf{P}_a=(\mathbf{I}-\mathbf{WH})\mathbf{P}_b
\]</div>
<p>The Kalman filter and OI methods are very similar with some important differences though. In the Kalman Filter, <span class="arithmatex">\(\mathbf{P}_b\)</span> is dynamic, hence updated with each analysis. In Optimal Interpolation (OI) <span class="arithmatex">\(\mathbf{P}_b\)</span> is static, hence constant in time. Due to the dynamic <span class="arithmatex">\(\mathbf{P}_b\)</span> in the Kalman Filter it is for example used partly in auto-piloting in airplanes and self-driving cars.</p>
<h3 id="the-kalman-filter-algorithm">The Kalman Filter Algorithm<a class="headerlink" href="#the-kalman-filter-algorithm" title="Permanent link">&para;</a></h3>
<p>The Kalman Filter algorithm is a recursive algorithm which has a prediction step and an update step.</p>
<p><mark class="critic">Prediction step</mark></p>
<div class="arithmatex">\[
\begin{align*}
\mathbf{x}_f=&amp;\mathbf{Mx}_a \\
\mathbf{P}_f=&amp;\mathbf{MP}_a\mathbf{M}^T+\mathbf{Q}
\end{align*}
\]</div>
<p><mark class="critic">Update step</mark></p>
<div class="arithmatex">\[
\begin{align*}
\mathbf{K}=&amp; \mathbf{P}_f\mathbf{H}^T(\mathbf{HP}_f\mathbf{H}^T+\mathbf{R})^{-1} \\
\mathbf{x}_a=&amp;\mathbf{x}_f+\mathbf{K}(\mathbf{y}-\mathbf{Hx}_f) \\
\mathbf{P}_a=&amp;(\mathbf{I}-\mathbf{KH})\mathbf{P}_f
\end{align*}
\]</div>
<p>Here <span class="arithmatex">\(\mathbf{K}=\mathbf{W}\)</span> is used for consistency with the literature. <span class="arithmatex">\(\mathbf{Q}\)</span> is the forecast error covariance. <span class="arithmatex">\(\mathbf{M}\)</span> is the linear tangent model and <span class="arithmatex">\(\mathbf{M}^T\)</span> is its adjoint. <span class="arithmatex">\(\mathbf{M}\)</span> is the operator that forwards the model in time from the analysis.</p>
<h2 id="optimal-interpolation-oi-equations">Optimal Interpolation (OI) equations<a class="headerlink" href="#optimal-interpolation-oi-equations" title="Permanent link">&para;</a></h2>
<p>Analysis Equation:</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{x}_a=\mathbf{x}_b+\mathbf{W}(\mathbf{y}-\mathbf{Hx}_b)=\mathbf{x}_b+\mathbf{Wd}
\end{align*}
\]</div>
<p>Optimal Weight:</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{W}=\mathbf{H}^T\mathbf{P}_b\mathbf{S}^{-1} = \frac{\mathbf{H}^T\mathbf{P}_b}{\mathbf{HP}_b\mathbf{H}^T+\mathbf{R}}
\end{align*}
\]</div>
<p>Analysis Error Covariance:</p>
<div class="arithmatex">\[
\begin{align*}
    \mathbf{P}_a = (\mathbf{I}-\mathbf{WH})\mathbf{P}_b
\end{align*}
\]</div>
<p><span class="arithmatex">\(\mathbf{P}_b\)</span> is static and is usually computed by running a model over a long period (weeks to months) and looking at the error statistics. Therefore <span class="arithmatex">\(\mathbf{P}_b\)</span> must be updated for every change in model configuration (dynamics, physics, domain).</p>
<h2 id="characteristic-overview-of-da-methods">Characteristic overview of DA methods<a class="headerlink" href="#characteristic-overview-of-da-methods" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th></th>
<th>Method</th>
<th style="text-align: center;"></th>
<th>Observations</th>
<th style="text-align: center;"></th>
<th>Covariance</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Variational</td>
<td style="text-align: center;">Kalman</td>
<td>Sequential</td>
<td style="text-align: center;">Smoother</td>
<td>Static</td>
<td style="text-align: center;">Dynamic</td>
</tr>
<tr>
<td>3DVar</td>
<td>x</td>
<td style="text-align: center;"></td>
<td>x</td>
<td style="text-align: center;"></td>
<td>x</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td>4DVar</td>
<td>x</td>
<td style="text-align: center;"></td>
<td></td>
<td style="text-align: center;">x</td>
<td>(x)</td>
<td style="text-align: center;">x</td>
</tr>
<tr>
<td>OI</td>
<td></td>
<td style="text-align: center;">x</td>
<td>x</td>
<td style="text-align: center;"></td>
<td>x</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td>KF</td>
<td></td>
<td style="text-align: center;">x</td>
<td>x</td>
<td style="text-align: center;"></td>
<td></td>
<td style="text-align: center;">x</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>https://www.universetoday.com/36302/atoms-in-the-universe/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>https://www.quora.com/How-much-hard-drive-storage-would-you-need-in-your-computer-to- fully-type-out-the-number-Googolplexian&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["header.autohide", "navigation.instant", "navigation.instant.progress", "navigation.sections", "navigation.tracking", "navigation.path", "navigation.top", "toc.follow", "toc.integrate", "content.code.copy"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../../javascripts/tablesort.min.js"></script>
      
        <script src="../../javascripts/tablesort.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="../../javascripts/tex-mml-chtml.js"></script>
      
        <script src="../../javascripts/polyfill.min.js"></script>
      
    
  </body>
</html>