{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"khintz.github.io","text":"<p>Welcome to my personal website. I am a scientific researcher at the Danish Meteorological Institute. My research focuses on the use of crowdsourced data for numerical weather prediction.</p> <p>I am focused on implementing ways of maintaining, improving and deploying code through the use of continuous integration and continuous deployment.</p> <p>I am also interested in the use of machine learning for weather forecasting.</p>"},{"location":"personal/publications/","title":"Publications","text":"<ul> <li> <p>Xiaohua Yang, Carlos Peralta, Bjarne Amstrup, Kasper Stener Hintz, S\u00f8ren Borg Thorsen, Leif Denby, Simon Kamuk Christiansen, Hauke Schulz, Sebastian Pelt, Mathias Schreiner, DANRA: The Kilometer-Scale Danish Regional Atmospheric Reanalysis, Arxiv (Pre-print)</p> <ul> <li>DOI: 10.48550/arXiv.2510.04681</li> <li>Link: 10.48550/arXiv.2510.04681</li> </ul> </li> <li> <p>Simon Adamov, Joel Oskarsson, Leif Denby, Tomas Landelius, Kasper Hintz, Simon Christiansen, Irene Schicker, Carlos Osuna, Fredrik Lindsten, Oliver Fuhrer, Sebastian Schemm, Building Machine Learning Limited Area Models: Kilometer-Scale Weather Forecasting in Realistic Settings, Arxiv (Pre-print)</p> <ul> <li>DOI: 10.48550/arXiv.2504.09340</li> <li>Link: https://doi.org/10.48550/arXiv.2504.09340</li> </ul> </li> <li> <p>Kasper S. Hintz, Callie McNicholas, Roger Randriamampianina, Hywel T. P. Williams, Bruce Macpherson, Marion Mittermaier, Jeanette Onvlee\u2010Hooimeijer, Bal\u00e1zs Szintai, Crowd\u2010sourced observations for short\u2010range numerical weather prediction: Report from EWGLAM/SRNWP Meeting 2019, Atmospheric Science Letters, 22, 6 (2021)</p> <ul> <li>DOI: 10.1002/asl.1031</li> <li>Link: https://doi.org/10.1002/asl.1031</li> </ul> </li> <li> <p>Hintz, K. S. and Vedel, H. and Kaas, E. and Nielsen, N. W. (2020), Estimation of Wind Speed and Roughness Length Using Smartphones: Method and Quality Assessment, J. Atmos. Oceanic Technol., 37, pp. 1319\u20131332</p> <ul> <li>DOI: 10.1175/JTECH-D-19-0037.1.</li> <li>Link: https://journals.ametsoc.org/jtech/article/37/8/1319/347960/Estimation-of-Wind-Speed-and-Roughness-Length</li> </ul> </li> <li> <p>Hintz, K. S., K. O\u2019Boyle, S. L. Dance, S. Al-Ali, I. Ansper, D. Blaauboer, M. Clark, A. Cress, M. Dahoui, R. Darcy, J. Hyrkkanen, L. Isaksen, E. Kaas, U. S. Korsholm, M. Lavanant, G. Le Bloa, E. Mallet, C. McNicholas, J. Onvlee-Hooimeijer, B. Sass, V. Siirand, H. Vedel, J. A. Waller, and X. Yang (2019), Collecting and utilising crowdsourced data for numerical weather prediction: Propositions from the meeting held in Copenhagen, 4\u20135 December 2018, Atmospheric Science Letters, 20 (7), e921</p> <ul> <li>DOI: 10.1002/asl.921.</li> <li>Link: https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/asl.921</li> </ul> </li> <li> <p>Hintz, K. S., H. Vedel, and E. Kaas (2019), Collecting and Processing of Barometric  Data from Smartphones for Potential Use in NWP Data Assimilation, Meteorological  Applications, 26, pp. 1\u201310</p> <ul> <li>DOI: 10.1002/met.1805.</li> <li>Link: https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/met.1805</li> </ul> </li> </ul>"},{"location":"personal/publications/#technical-reports","title":"Technical Reports","text":"<ul> <li> <p>Nielsen, N. W., Amstrup B., and K. S. Hintz (2016), Sensitivity study of visibility forecasts based on modifications to the visibility scheme in the nowcasting model at DMI, DMI Report No. 16-23, Danish Meteorological Institute.</p> <ul> <li>Link: https://www.dmi.dk/fileadmin/user_upload/Rapporter/TR/2016/fog_tst_v3.pdf</li> </ul> </li> <li> <p>Xiaohua Yang, Kasper Stener Hintz, Carlos Peralta Aros, Bjarne Amstrup, (2021), DMI Report 21-31 Danish Regional Atmospheric Reanalysis. Final scientific report of the 2020 National Centre for Climate Research Work Package 3.2.1, Regional Reanalysis Pilot.</p> <ul> <li>Link: https://www.dmi.dk/fileadmin/Rapporter/2021/DMI_Report_21-31.pdf</li> </ul> </li> </ul>"},{"location":"topics/dataassimilation/","title":"Data Assimilation","text":""},{"location":"topics/dataassimilation/#introduction","title":"Introduction","text":"<p>Data Assimilation (DA) is the art of combining information from different sources in an optimal way. In climate and NWP, these sources are a first guess and observations with the aim of improving an estimate of the state of a system. DA has two main objectives.</p> <p>1) Produce an initial state from which we can start our forecast. 2) Quantifying the uncertainty of the initial state.</p> <p>From the initial state of the atmosphere the model integrates a set of prognostic equations forward in time. This is prediction.</p>"},{"location":"topics/dataassimilation/#what-is-the-optimalbest-analysis-in-operational-nwp","title":"What is the optimal/best analysis in operational NWP?","text":"<ul> <li>The best analysis is the one that leads to the best forecast, not necessarily the analysis closest to the true state.</li> <li>Computational efficient: Depending on operational setup you should make your analysis in 30-60 minutes. Even your smartphones can compute a reasonable weather forecast faster than the weather evolves.</li> <li>Minimises error/maximises probability.</li> </ul>"},{"location":"topics/dataassimilation/#jargon-in-da","title":"Jargon in DA","text":"<p>Our prior information is a First Guess (\\(\\mathbf{x_g}\\)) and some Observations (\\(\\mathbf{y}\\)). \\(\\mathbf{x_g}\\) is the prior information and can for example be:</p> <ul> <li>A random guess (bad choice!)</li> <li>A climatological field (better, but room for improvement)</li> <li>The analysis from a previous cycle \u2217 A short term forecast from the previous cycle \\(\\mathbf{x_b}\\)</li> <li>Most often \\(\\mathbf{x_g}=\\mathbf{x_b}\\) in operational setups.</li> </ul> <p>We shall refer to a short term forecast as the background field \\(\\mathbf{x_b}\\), and denote our analysis as \\(\\mathbf{x_a}\\)</p>"},{"location":"topics/dataassimilation/#observation-coveragedensity","title":"Observation coverage/density","text":"<p>The DMI Harmonie model, \"NEA\", has about \\(10^9\\) prognostic variables that we need to assign an initial value to produce a forecast. We have far less observations (\\(\\approx 10^4\\) to \\(10^6\\)) giving rise to an insufficient data coverage. Furthermore, observations are:</p> <ul> <li>Not evenly distributed in space and time</li> <li>Not taken exactly at the grid points.</li> <li>Not perfect - they contain errors. Quality control is needed.</li> </ul> <p>An observation operator is needed to interpolate and convert to state variables if needed.</p>"},{"location":"topics/dataassimilation/#a-refresher-on-statistics","title":"A refresher on statistics","text":"<p>Suppose we have the noon-day pressure \\(p_i\\) and temperature \\(T_i\\) at Copenhagen, every day for a year. Let \\(n=365\\). The mean pressure, \\(\\overline{p}\\), is defined to be</p> \\[ \\overline{p}=\\mathbb{E}(p)=\\frac{1}{n}\\sum_{i=1}^{n}p_i \\] <p>and similarly for the mean temperature \\(\\overline{T}\\).</p> <p>The variance of pressure, \\(\\sigma_p^2\\), is defined as</p> \\[ \\sigma_p^2=\\mathbb{E}((p-\\overline{p})^2)=\\frac{1}{n}\\sum_{i=1}^{n}(p_i-\\overline{p})^2 \\] <p>and similarly for variance of the temperature \\(\\sigma_T^2\\).</p> <p>The standard deviations, \\(\\sigma_p\\) and \\(\\sigma_T\\) are the square roots of the variances. They measure the root mean square deviation from the mean.</p>"},{"location":"topics/dataassimilation/#framework-of-da","title":"Framework of DA","text":"<p>As has been described we have information both from a short term forecast, \\(\\mathbf{x_b}\\), and some observations \\(\\mathbf{y}\\). We need to develop a framework for combining the two sources of information.</p> <p>Luckily this framework already exist! \u2192 Bayes Theorem</p> \\[ \\text{pdf}(\\mathbf{x}|\\mathbf{y})=\\frac{\\text{pdf}(\\mathbf{y}|\\mathbf{x})\\text{pdf}(\\mathbf{x})}{\\text{pdf}(\\mathbf{y})} \\] <ul> <li>\\(\\text{pdf}(\\mathbf{x}|\\mathbf{y})\\) is the posterior probability density function (pdf) of \\(\\mathbf{x}\\) given \\(\\mathbf{y}\\)</li> <li>\\(\\text{pdf}(\\mathbf{y}|\\mathbf{x})\\) is the likelihood function, the \\(\\text{pdf}\\) of the observations given the state variables.</li> <li>\\(\\text{pdf}(\\mathbf{x})\\) is the prior \\(\\text{pdf}\\) of the state variables coming from the background field (model)</li> <li>\\(\\text{pdf}(\\mathbf{y})\\) is the evidence. It is used as a normalisation constant and often not computed explicitly so we will ignore this for now.</li> </ul> <p>Combining information in the form of the prior and the likehood gives us a more narrow posterior \\(\\text{pdf}\\). Note that as long one knows the associated error of either the model or observations the posterior will always gain information.</p> <p> </p> Probability Density Functions"},{"location":"topics/dataassimilation/#reality-bites","title":"Reality bites","text":"<p>Making no approximations - considering the full non-linear DA problem - we have to find the joint \\(\\text{pdf}\\) of all variables, that is the probabilities for all possible combinations of all of our variables.</p> <p>Assume we only need 10 bins for each variable to generate a joint \\(\\text{pdf}\\) and assume we have a small model of only \\(10^6\\) variables. Then we need to store \\(10^{1000000}\\) numbers.</p> <p>There are approximately \\(10^{80}\\) atoms in the universe<sup>1</sup>. So data assimilation is much larger than the universe! We need \\(10^{52}\\) solar system sized hard drives to store just a googol (\\(10^{100}\\)) bytes, but we only have about \\(10^{24}\\) stars<sup>2</sup>. Approximations and optimisations are indeed needed.</p> <p>The key here is that we can\u2019t determine the pdf\u2019s in large dimensional systems.</p>"},{"location":"topics/dataassimilation/#approximate-solutions","title":"Approximate solutions","text":"<p>Estimating the \\(\\text{pdf}\\)\u2019s in large dimensional systems, such as in NWP, is practically impossible!</p> <p>Approximate solutions of Bayes theorem leads to data assimilation methods.</p> <ul> <li>Variational methods: Solves for the mode of the posterior pdf.</li> <li>Kalman-based methods: Solves for the mean and covariance of posterior pdf.</li> <li>Particle Filters: Finds a sample representaion of the posterior pdf.</li> </ul> <p>Variational methods and Kalman-based methods both assume errors to be Gaussian. Particle Filters, however have no such assumptions.</p>"},{"location":"topics/dataassimilation/#variational-methods","title":"Variational Methods","text":"<p>The variational methods assume errors to be Gaussian. This is convenient as the pdf is completely determined by the mean and covariance. But it is also a strong constraint. Do the errors, in fact, follow a Gaussian?</p> <p>In geophysics, we often deal with only the positive real axis, where we then often find a tail on the error distribution. For example precipitation, wind, salinity etc.</p> <p>Let us see how the variational approach works with a scalar example of temperature observations.</p> <p>We will feed our own \u201dtoy data assimilation system\u201d the observations, to obtain the most likely temperature given your observations.</p>"},{"location":"topics/dataassimilation/#the-cost-function","title":"The cost function","text":"<p>We assume that your observations has been drawn from a Gaussian distribution.</p> \\[ p(T_1|T)=\\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\left(-\\frac{(T_1-T)^2}{2\\sigma_1^2}\\right) \\] <p>and likewise for \\(T_2\\). We can express the joint probability by multiplying the two probabilities together.</p> \\[ p(T_1,T_2|T)=\\frac{1}{2\\pi\\sigma_1\\sigma_2}\\exp\\left(-\\frac{(T_1-T)^2}{2\\sigma_1^2}-\\frac{(T_2-T)^2}{2\\sigma_2^2}\\right) \\] <p>This is the same as the likelihood for \\(T\\) given \\(T_1\\) and \\(T_2\\), \\(L(T|T_1,T_2)\\). To find the most likely \\(T\\), which will be our analysis temperature \\(T_a\\), we want to maximise the likelihood given \\(T_1\\) and \\(T_2\\).</p> \\[ \\text{max}[L(T|T_1,T_2)]=\\text{max}\\left[\\frac{1}{2\\pi\\sigma_1\\sigma_2}\\exp\\left(-\\frac{(T_1-T)^2}{2\\sigma_1^2}-\\frac{(T_2-T)^2}{2\\sigma_2^2}\\right)\\right] \\] <p>To make things easier, we take the logarithmic of this expression. Note that the logarithmic is a monotonic function, so maximising the logarithmic of the function is equivalent to maximising the function itself.</p> \\[ \\text{max}[\\ln L(T|T_1,T_2)]=\\text{max}\\left[\\text{const}-\\frac{1}{2}\\left(\\frac{(T-T_1)^2}{\\sigma_1^2}+\\frac{(T-T_2)^2}{\\sigma_2^2}\\right)\\right] \\] <p>Note that maximising the function is equivalent to minimising the last term on the right-hand-side. For our \u201dtwo-temperature-problem\u201d this is defined as our cost function.</p> \\[ J=\\frac{1}{2}\\left(\\frac{(T-T_1)^2}{\\sigma_1^2}+\\frac{(T-T_2)^2}{\\sigma_2^2}\\right) \\] <p>Minimising \\(J\\) corresponds to maximising the likelihood of \\(T\\) given \\(T_1\\) and \\(T_2\\).</p> <p>To minimise \\(J\\) to find our analysis temperature using the variational approach, we will start by guessing some value of \\(T\\) and explore space. Different algorithms exist for this such as \"steepest-descent\".</p> Example <pre><code>\"\"\"\n'Toy-Tool' to play with a very simple scalar case of the cost function\nin variational data assimilation.\n\nGiven two guesses of temperature, we try to find the most likely\ntrue temperature by minimizing a cost function.\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define variables\nT1 = 21.6  # First guess of temperature\nsigma1 = 1.8  # Standard deviation for T1\n\nT2 = 23.4  # Second guess of temperature\nsigma2 = 0.8  # Standard deviation for T2\n\n# Initial guess (mean of T1 and T2)\nT0 = (T1 + T2) / 2\n\n# Initialize variables\nT = T0  # Current temperature guess\neps = 0.1  # Epsilon for perturbation\nJ0 = 2000.  # Initial cost value\n\n# Lists to store iteration data\niteration = []\nTa = []\ncostF = []\n\n# Perform 100 iterations\nfor k in range(100):\n    direction = np.random.randint(-1, 2) + eps\n    size_direction = np.random.rand(1) * direction\n\n    Tg = T + size_direction  # New temperature guess\n    J = 1/2 * ((Tg - T1)**2 / sigma1**2 + (Tg - T2)**2 / sigma2**2)  # Cost function\n\n    # Update if new cost is lower\n    if J &lt; J0 and k &gt; 0:\n        J0 = J\n        T = Tg\n        iteration.append(k)\n        Ta.append(T)\n        costF.append(J)\n        print(T, J)\n\n# Plotting\nfig = plt.figure()\nplt.plot(Ta, iteration)\nplt.gca().invert_yaxis()\nplt.ylabel('Iteration')\nplt.xlabel('T [C]')\nplt.show()\n</code></pre>"},{"location":"topics/dataassimilation/#probability-distributions-for-the-multivariate-case","title":"Probability distributions for the multivariate case","text":"<p>The principles we have just seen, are the same in the multivariate case.</p> <p>The prior</p> \\[ p(\\mathbf{x})\\propto\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{x_b})^T\\mathbf{P}^{-1}(\\mathbf{x}-\\mathbf{x_b})\\right) \\] <p>The likelihood</p> \\[ p(\\mathbf{y}|\\mathbf{x})\\propto\\exp\\left(-\\frac{1}{2}(\\mathbf{y}-H(\\mathbf{x}))^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}))\\right) \\] <p>The posterior</p> \\[ p(\\mathbf{x}|\\mathbf{y})\\propto\\exp\\left(-\\frac{1}{2}[(\\mathbf{x}-\\mathbf{x_b})^T\\mathbf{P}_b^{-1}(\\mathbf{x}-\\mathbf{x_b})+(\\mathbf{y}-H(\\mathbf{x}))^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}))]\\right) \\]"},{"location":"topics/dataassimilation/#notation","title":"Notation","text":"<p>\\(\\mathbf{x}\\): State vector of size \\(n\\)</p> <p>\\(\\mathbf{x_b}\\): Background state vector of size \\(n\\)</p> <p>\\(\\mathbf{P}_b\\) or \\(\\mathbf{B}\\): Background error covariance matrix of size \\(n\\times n\\)</p> <p>\\(\\mathbf{y}\\): Observation vector of size \\(p\\)</p> <p>\\(\\mathbf{R}\\): Observation error covariance matrix of size \\(p\\times p\\)</p> <p>\\(n\\): Total number of grid points \\(\\times\\) number of model variables (\\(\\approx 10^7\\))</p> <p>\\(p\\): Total number of observations (\\(\\approx 10^4\\))</p> <p>\\(H(\\mathbf{x})\\): Observation operator that maps model space to observation space. \\(H(x_i)\\) is the models estimate on \\(y_i\\). \\(H\\) can be non-linear (eg. radiance measurements) or linear (eq. synop temperature measurements).</p>"},{"location":"topics/dataassimilation/#cost-function-multivariate-case","title":"Cost function multivariate case","text":"<p>Taking the term in the brackets of the posterior gives us the cost function that we want to minimise to maximise the posterior.</p> \\[ J(\\mathbf{x})=\\frac{1}{2}[(\\mathbf{x}-\\mathbf{x_b})^T\\mathbf{P}_b^{-1}(\\mathbf{x}-\\mathbf{x_b})+(\\mathbf{y}-H(\\mathbf{x}))^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}))] \\] <p>where \\(\\mathbf{x}\\) is the state vector with all variables. The \\(\\mathbf{x}\\) that minimises the cost function, \\(J\\) is our analysis state vector \\(\\mathbf{x}_a\\). \\(\\mathbf{P}_b\\) is the background error covariance matrix. \\(\\mathbf{P}_b\\) is sometimes also denoted \\(\\mathbf{B}\\) in the literature. Here I use \\(\\mathbf{P}_b\\) for consistency with the Kalman Filter algorithm (to be introduced)</p> <p>The cost function cannot be computed directly. Different assumptions leads to either 3DVar or 4DVar.</p> <p>\\(\\mathbf{P}_b\\) is a huge matrix! (\\(\\approx 10^7\\times10^7\\)) - We can\u2019t store this on a computer, so we are forced to simplify it. Furthermore, it is not given that we have all the information needed to determine all of its elements.</p> <p>We dont know \\(\\mathbf{P}_b\\). In general \\(\\mathbf{P}_b = \\text{cov}[\\mathbf{x}_t \u2212 \\mathbf{x}_b]\\), but we have no way to know \\(\\mathbf{x}_t\\), therefore a proxy is needed for \\(\\mathbf{x}_t\\).</p> <p>As a proxy for \\(\\mathbf{x}_t\\) one can use \u201dobservation-minus-background\u201d statistics, by running the model for a long period and see how the error looks in average.</p> <p>\\(\\mathbf{P}_b\\) is essential. Its role is to spread out information from observations. How should a pressure observation in Copenhagen affect variables in Oslo? Also, it ensures dynamically consistent increments in other model variables (How should a temperature increase affect the wind?)</p> Tip <p>Good To Know: \\(\\mathbf{P}_b\\) is often referred to as \u201dstructure functions\u201d in the literature.</p> <p> </p> Increment of assimilating a single pressure observation <p>In the figure the difference (increment) between to analysis states is shown. A single additional pressure observation is assimilated in the second analysis. The increment is largest close to the observation and decreases with distance. The spread of the increment is determined by \\(\\mathbf{P}_b\\).</p>"},{"location":"topics/dataassimilation/#solving-for-the-gradient-of-the-cost-function","title":"Solving for the gradient of the cost function","text":"<p>The minimum of the cost function is obtained for \\(\\mathbf{x}=\\mathbf{x}_a\\), e.g. the solution of</p> \\[ \\nabla J(\\mathbf{x}_a)=0 \\] <p>So we wish to solve for the gradient of the cost function. To simplify the problem we linearise the observation operator \\(H\\) as</p> \\[ H(\\mathbf{x})\\approx \\nabla H(\\mathbf{x}_b)\\cdot\\delta \\mathbf{x}+H(\\mathbf{x}_b) \\] <p>and assume that the analysis is close to the truth so that we can write</p> \\[ x=\\mathbf{x}_b+(\\mathbf{x}-\\mathbf{x}_b) \\] <p>assuming \\(\\mathbf{x}-\\mathbf{x}_b\\) is small, \\(\\mathbf{y}-H(\\mathbf{x})\\) can be written as</p> \\[ \\begin{align*} [\\mathbf{y}-H(\\mathbf{x})]=\\mathbf{y}-H(\\mathbf{x}_b+(\\mathbf{x}-\\mathbf{x}_b))\\\\ =\\mathbf{y}-H(\\mathbf{x})-\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b) \\end{align*} \\] <p>This is an advantage as \\(H(\\mathbf{x}_b)\\) and \\(\\mathbf{x}\\) is known a priori. The cost function can now be written as</p> \\[ \\begin{align*} J(\\mathbf{x})=&amp;\\frac{1}{2}[(\\mathbf{x}-\\mathbf{x}_b)^T\\mathbf{P}_b^{-1}(\\mathbf{x}-\\mathbf{x}_b)+[(\\mathbf{y}-H(\\mathbf{x}_b)-\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b)]^T\\mathbf{R}^{-1} \\\\ &amp;[(\\mathbf{y}-H(\\mathbf{x}_b)-\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b)])]] \\end{align*} \\] <p>If we then assume that \\(\\mathbf{R}\\) is symmetric so that \\(\\mathbf{HR}^{-1}=\\mathbf{R}^{-1}\\mathbf{H}\\) and expanding the brackets we get</p> \\[ \\begin{align*} J(\\mathbf{x})=&amp;\\frac{1}{2}[(\\mathbf{x}-\\mathbf{x}_b)^T\\mathbf{P}_b^{-1}(\\mathbf{x}-\\mathbf{x}_b)+(\\mathbf{x}-\\mathbf{x}_b)^T\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b) \\\\ &amp;-(\\mathbf{y}-H(\\mathbf{x}_b))^T\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b)-(\\mathbf{x}-\\mathbf{x}_b)^T\\mathbf{H}^T\\mathbf{R}^{-1} \\\\ &amp;(\\mathbf{y}-H(\\mathbf{x}_b))+(\\mathbf{y}-H(\\mathbf{x}_b))^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}_b))] \\end{align*} \\] <p>If we combine the first two terms we get</p> \\[ \\begin{align*} 2J(\\mathbf{x})=&amp;\\ (\\mathbf{x}-\\mathbf{x}_b)^T[\\mathbf{P}_b^{-1}+\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{H}](\\mathbf{x}-\\mathbf{x}_b) \\\\ &amp;-(\\mathbf{y}-H(\\mathbf{x}_b))^T\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b) \\\\ &amp;-(\\mathbf{x}-\\mathbf{x}_b)^T\\mathbf{H}^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}_b)) \\\\ &amp;+\\text(Term\\ independent\\ on\\ \\mathbf{x}) \\\\ =&amp;\\ (\\mathbf{x}-\\mathbf{x}_b)^T[\\mathbf{P}_b^{-1}+\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{H}](\\mathbf{x}-\\mathbf{x}_b) \\\\ &amp;-2(\\mathbf{y}-H(\\mathbf{x}_b))^T\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_b) \\\\ &amp;+\\text(Term\\ independent\\ on\\ \\mathbf{x}) \\end{align*} \\] <p>Given a quadratic function \\(F(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^T\\mathbf{Ax}+\\mathbf{d}^T\\mathbf{x}+c\\) the gradient is given by \\(\\nabla F(\\mathbf{x})=\\mathbf{Ax}+\\mathbf{d}\\). Using this and setting \\(\\nabla J(\\mathbf{x})=0\\) to ensure \\(J\\) is a minimum (though it could be a maximum or a saddle point) we obtain an analytical expression for the analysis state vector \\(\\mathbf{x}_a\\).</p> \\[ \\begin{align*} \\mathbf{x}_a = \\mathbf{x}_b+(\\mathbf{P}_b^{-1}+\\mathbf{H}^T\\mathbf{R}^{-1}\\mathbf{H})^{-1}\\mathbf{H}^T\\mathbf{R}^{-1}(\\mathbf{y}-H(\\mathbf{x}_b)) \\end{align*} \\] <p>The is the analytical solution to 3DVar. In practice it is solved by an iterative method such as the steepest descent method, as we saw in the scalar example. Unfortunately for us, it is impossible to invert such huge matrices as \\(\\mathbf{P}_b\\) so we need to find approximations. Also 3DVar assumes all observations to be taken at the time of the analysis. This is not the case in reality. \\(\\mathbf{P}_b\\) is also assumed to be constant in time, which is not the case in reality (not allowed to evolve dynamically).</p> <p> </p> Schematic figure of the 3DVar algorithm"},{"location":"topics/dataassimilation/#kalman-based-methods","title":"Kalman-based Methods","text":"<p>Optimal Interpolation, Kalman Filter and Kalman Ensemble filters are methods that are widely used in operational centers and they are all based on the Kalman equations, which we shall derive to broaden our understanding of the methods.</p> <p>An analysis is found by using a least square approach in the sense that we find an \u2019optimal\u2019 analysis by minimising the errors.</p> <p>We write our analysis \\(\\mathbf{x}_a\\) as a linear combination of the background, \\(\\mathbf{x}_b\\) and some observations \\(\\mathbf{y}\\) as</p> \\[ \\mathbf{x}_a=\\mathbf{Lx}_b+\\mathbf{Wy} \\] <p>where \\(\\mathbf{L}\\) and \\(\\mathbf{W}\\) are weights that we need to find.</p> <p>One can derive the least square solution from this equation and at the same time get rid of one of the weights. This is a statistical approach trying to minimise the errors of the analysis.</p> <p>The errors of \\(\\mathbf{x}_a\\) and \\(\\mathbf{x}_b\\) can be written as</p> \\[ \\begin{align*} \\mathbf{e}_a=&amp;\\ \\mathbf{x}_a-\\mathbf{x}_t \\\\ \\mathbf{e}_b=&amp;\\ \\mathbf{x}_b-\\mathbf{x}_t \\end{align*} \\] <p>where \\(\\mathbf{x}_t\\) is the true (unknown) state. A linear observation process can be defined as</p> \\[ \\mathbf{y}=\\mathbf{Hx}_t+\\mathbf{b}_0 \\] <p>where \\(\\mathbf{H}\\) is a matrix representing a linear transformation between the true variables into the observed ones (also called a forward operator) and \\(\\mathbf{b}_0\\) is the observational error.</p> <p>Assume that the observation error have zero mean and covariance \\(\\mathbf{R}\\),</p> \\[ \\begin{align*} \\mathbb{E}(\\mathbf{b}_0) =&amp; 0 \\\\ \\mathbb{E}(\\mathbf{b}_0\\mathbf{b}_0^T) =&amp; \\mathbf{R}_k\\delta_{kk'} \\end{align*} \\] <p>where \\(\\delta_{kk'}\\) is the dirac-delta function.</p> <p>Also assume that the observations errors and model errors are uncorrelated</p> \\[ \\mathbb{E}(\\mathbf{b}_t\\mathbf{b}_0^T)=0 \\] <p>Substitung the errors of the analysis and background into our analysis equation and subtracting \\(\\mathbf{x}_t\\) we get</p> \\[ \\mathbf{e}_a=\\underbrace{\\mathbf{Le}_b}_{\\text{Background Error}}+\\underbrace{\\mathbf{Wb}_0}_{\\text{Observational Error}}+\\underbrace{(\\mathbf{L}+\\mathbf{WH}-\\mathbf{I})\\mathbf{x}_t}_{\\text{Bias}} \\] <p>Assuming that the forecast error is unbiased (\\(\\mathbb{E}(\\mathbf{e}_b)=\\mathbb{E}(\\mathbf{x}_b-\\mathbf{x}_t)=0\\)), the condition \\((\\mathbf{L}+\\mathbf{WH}-\\mathbf{I})\\mathbb{E}(\\mathbf{x}_t)=0\\) must be met. In general \\(\\mathbb{E}(\\mathbf{x}_t)\\neq 0\\) so to obtain an unbiased analysis we can write the first weight in terms of the second as</p> \\[ \\mathbf{L}=\\mathbf{I}-\\mathbf{WH} \\] <p>Substituting this into the analysis equation we get the Kalman analysis equation</p> \\[ \\begin{align*} \\mathbf{x}_a=&amp;\\mathbf{x}_b+\\mathbf{W}\\underbrace{(\\mathbf{y}-\\mathbf{Hx}_b)}_{\\text{Innovation}} \\\\ =&amp;\\mathbf{x}_b+\\mathbf{Wd} \\end{align*} \\]"},{"location":"topics/dataassimilation/#dimension","title":"Dimension","text":"<p>\\(n=\\text{Total number of grid points}\\times\\text{number of model variables}\\)</p> <p>\\(\\mathbf{x}\\): State vector of size \\(n\\)</p> <p>\\(\\mathbf{W}\\): Weight matrix of size \\(p\\times n\\) where \\(p\\) is the number of observations</p> <p>\\(\\mathbf{y}\\): Observation vector of size \\(p\\)</p> <p>\\(\\mathbf{H}\\): Matrix of size \\(n\\times p\\)</p>"},{"location":"topics/dataassimilation/#derivation-of-the-weight","title":"Derivation of the weight","text":"<p>At this point \\(\\mathbf{W}\\) is still unknown to us. \\(\\mathbf{W}\\) chosen such that the variances are minimised. Consider the error covariance of the analysis,</p> \\[ \\begin{align*}     \\mathbf{P}_a=\\text{cov}[\\mathbf{x}_t-\\mathbf{x}_a]=     \\left[ {\\begin{array}{cccc}     \\sigma_{1,1}^2 &amp; \\sigma_{1,2}^2 &amp; \\dots &amp; \\sigma_{1,n}^2 \\\\     \\sigma_{2,1}^2 &amp; \\sigma_{2,2}^2 &amp; \\dots &amp; \\sigma_{2,n}^2 \\\\     \\vdots         &amp; \\vdots         &amp; \\ddots &amp; \\vdots \\\\     \\sigma_{m,1}^2 &amp; \\sigma_{m,2}^2 &amp; \\dots &amp; \\sigma_{m,n}^2     \\end{array} } \\right] \\end{align*} \\] <p>Note that the variances are the trace of the error covariance matrix. This can be expanded by using the Kalman analysis equation and \\(\\mathbf{y}=\\mathbf{Hx}_t+\\mathbf{b}_0\\) to get</p> \\[ \\begin{align*}     \\mathbf{P}_a = \\text{cov}[(\\mathbf{I}-\\mathbf{WH})(\\mathbf{x}_t-\\mathbf{x}_b)-\\mathbf{Wb}_0]. \\end{align*} \\] <p>This can be simplified by using the covariance matrix identity, \\(\\text{cov}(\\mathbf{AB})=\\mathbf{A}\\text{cov}(\\mathbf{B})\\mathbf{A}^T\\):</p> \\[ \\begin{align*}     \\mathbf{P}_a = (\\mathbf{I}-\\mathbf{WH})\\mathbf{P}_b(\\mathbf{I}-\\mathbf{WH})^T+\\mathbf{WRW}^T, \\end{align*} \\] <p>where \\(\\mathbf{P}_b=\\text{cov}(\\mathbf{x}_t-\\mathbf{x}_b)\\) and \\(\\mathbf{R}=\\text{cov}(\\mathbf{b}_0)\\).</p> <p>We expand by using that \\(\\mathbf{I}=\\mathbf{I}^T\\) and defining \\(\\mathbf{S}=\\mathbf{HP}_b\\mathbf{H}^T+\\mathbf{R}\\) to get</p> \\[ \\begin{align*}     \\mathbf{P}_a = \\mathbf{P}_b - \\mathbf{W}^T\\mathbf{H}^T\\mathbf{P}_b-\\mathbf{WHP}_b+\\mathbf{WSW}^T. \\end{align*} \\] <p>Recall that we want to minimise the trace of the error covariance matrix (to minimise the variances). We take the derivative of the trace of \\(\\mathbf{P}_a\\) with respect to \\(\\mathbf{W}\\) and setting it equal to 0 to find the minimum (hint: Use the matrix identity \\(\\nabla_A\\text{Tr}(\\mathbf{AB})=\\mathbf{B}^T\\)).</p> \\[ \\begin{align*}     \\frac{\\partial\\text{Tr}(\\mathbf{P}_a)}{\\partial\\mathbf{W}} = -2(\\mathbf{HP}_b)^T + 2\\mathbf{WS} \\equiv 0. \\end{align*} \\] <p>Using that \\(\\mathbf{P}_b\\) is symetric (\\(\\mathbf{P}_b=\\mathbf{P}_b^T\\)) and solving for \\(\\mathbf{W}\\) yields the optimal weight</p> \\[ \\begin{align*}     \\mathbf{W}=\\mathbf{H}^T\\mathbf{P}_b\\mathbf{S}^{-1} = \\frac{\\mathbf{H}^T\\mathbf{P}_b}{\\mathbf{HP}_b\\mathbf{H}^T+\\mathbf{R}} \\end{align*} \\] <p>This is called the Kalman gain and is the optimal weight that minimises the variances of the analysis.</p>"},{"location":"topics/dataassimilation/#understanding-the-kalman-gain","title":"Understanding the Kalman gain","text":"<p>It is important to get an intuitive understanding of the Kalman gain. The Kalman gain is a measure of how much we trust the observations. If the observation error is large, the Kalman gain will be small and vice versa. If the background error is large, the Kalman gain will be small and vice versa.</p> <p>Recall that \\(\\mathbf{R}\\) is the observational error covariance matrix and \\(\\mathbf{P}_b\\) is the background error covariance matrix.</p> <p>If the observational error is much larger than the model error the Kalman gain will go towards 0, making the innovation term in equation small, such that observations are given a low weight.</p> <p>On the other hand, if the model error is much larger than the observational error the weight will go towards 1, given the innovation term a high weight in the analysis equation.</p> <p>One advantage of the Kalman system is that we get the error of the analysis directly by computing \\(\\mathbf{P}_a\\). This is not the case for the variational methods. However, we can now simplify the equation for \\(\\mathbf{P}_a\\) by multiplying the Kalman gain with \\(\\mathbf{W}^T\\mathbf{S}\\) to get</p> \\[ \\mathbf{W}^T\\mathbf{SW} = \\mathbf{P}_b\\mathbf{H}^T\\mathbf{W}^T \\] <p>Substituting this into the equation for \\(\\mathbf{P}_a\\) we get</p> \\[ \\mathbf{P}_a=(\\mathbf{I}-\\mathbf{WH})\\mathbf{P}_b \\] <p>The Kalman filter and OI methods are very similar with some important differences though. In the Kalman Filter, \\(\\mathbf{P}_b\\) is dynamic, hence updated with each analysis. In Optimal Interpolation (OI) \\(\\mathbf{P}_b\\) is static, hence constant in time. Due to the dynamic \\(\\mathbf{P}_b\\) in the Kalman Filter it is for example used partly in auto-piloting in airplanes and self-driving cars.</p>"},{"location":"topics/dataassimilation/#the-kalman-filter-algorithm","title":"The Kalman Filter Algorithm","text":"<p>The Kalman Filter algorithm is a recursive algorithm which has a prediction step and an update step.</p> <p>Prediction step</p> \\[ \\begin{align*} \\mathbf{x}_f=&amp;\\mathbf{Mx}_a \\\\ \\mathbf{P}_f=&amp;\\mathbf{MP}_a\\mathbf{M}^T+\\mathbf{Q} \\end{align*} \\] <p>Update step</p> \\[ \\begin{align*} \\mathbf{K}=&amp; \\mathbf{P}_f\\mathbf{H}^T(\\mathbf{HP}_f\\mathbf{H}^T+\\mathbf{R})^{-1} \\\\ \\mathbf{x}_a=&amp;\\mathbf{x}_f+\\mathbf{K}(\\mathbf{y}-\\mathbf{Hx}_f) \\\\ \\mathbf{P}_a=&amp;(\\mathbf{I}-\\mathbf{KH})\\mathbf{P}_f \\end{align*} \\] <p>Here \\(\\mathbf{K}=\\mathbf{W}\\) is used for consistency with the literature. \\(\\mathbf{Q}\\) is the forecast error covariance. \\(\\mathbf{M}\\) is the linear tangent model and \\(\\mathbf{M}^T\\) is its adjoint. \\(\\mathbf{M}\\) is the operator that forwards the model in time from the analysis.</p>"},{"location":"topics/dataassimilation/#optimal-interpolation-oi-equations","title":"Optimal Interpolation (OI) equations","text":"<p>Analysis Equation:</p> \\[ \\begin{align*}     \\mathbf{x}_a=\\mathbf{x}_b+\\mathbf{W}(\\mathbf{y}-\\mathbf{Hx}_b)=\\mathbf{x}_b+\\mathbf{Wd} \\end{align*} \\] <p>Optimal Weight:</p> \\[ \\begin{align*}     \\mathbf{W}=\\mathbf{H}^T\\mathbf{P}_b\\mathbf{S}^{-1} = \\frac{\\mathbf{H}^T\\mathbf{P}_b}{\\mathbf{HP}_b\\mathbf{H}^T+\\mathbf{R}} \\end{align*} \\] <p>Analysis Error Covariance:</p> \\[ \\begin{align*}     \\mathbf{P}_a = (\\mathbf{I}-\\mathbf{WH})\\mathbf{P}_b \\end{align*} \\] <p>\\(\\mathbf{P}_b\\) is static and is usually computed by running a model over a long period (weeks to months) and looking at the error statistics. Therefore \\(\\mathbf{P}_b\\) must be updated for every change in model configuration (dynamics, physics, domain).</p>"},{"location":"topics/dataassimilation/#characteristic-overview-of-da-methods","title":"Characteristic overview of DA methods","text":"Method Observations Covariance Variational Kalman Sequential Smoother Static Dynamic 3DVar x x x 4DVar x x (x) x OI x x x KF x x x <ol> <li> <p>https://www.universetoday.com/36302/atoms-in-the-universe/\u00a0\u21a9</p> </li> <li> <p>https://www.quora.com/How-much-hard-drive-storage-would-you-need-in-your-computer-to- fully-type-out-the-number-Googolplexian\u00a0\u21a9</p> </li> </ol>"}]}